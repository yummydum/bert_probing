# xlm
python ./examples/run_glue.py  \
--model_type xlm     --model_name_or_path xlm-mlm-enfr-1024  --task_name $TASK_NAME  \
--do_train     --do_eval     --do_lower_case     --data_dir $GLUE_DIR/$TASK_NAME  \
--max_seq_length 128     --per_gpu_eval_batch_size=8 --per_gpu_train_batch_size=8  \
--learning_rate 2e-5     --num_train_epochs 3.0  \
--output_dir ~/bert_probing/data/xlm/$TASK_NAME/

# XLnet
python ./examples/run_glue.py  \
--model_type xlnet     --model_name_or_path bert-base-uncased     --task_name $TASK_NAME  \
--do_train     --do_eval     --do_lower_case     --data_dir $GLUE_DIR/$TASK_NAME  \
--max_seq_length 128     --per_gpu_eval_batch_size=8 --per_gpu_train_batch_size=8  \
--learning_rate 2e-5     --num_train_epochs 3.0  \
--output_dir ~/bert_probing/data/xlnet/$TASK_NAME/

# roberta
python ./examples/run_glue.py  \
--model_type roberta    --model_name_or_path roberta-base  --task_name $TASK_NAME  \
--do_train     --do_eval     --do_lower_case     --data_dir $GLUE_DIR/$TASK_NAME  \
--max_seq_length 128     --per_gpu_eval_batch_size=8  --per_gpu_train_batch_size=8  \
--learning_rate 2e-5     --num_train_epochs 3.0  \
--output_dir ~/bert_probing/data/roberta/$TASK_NAME/
